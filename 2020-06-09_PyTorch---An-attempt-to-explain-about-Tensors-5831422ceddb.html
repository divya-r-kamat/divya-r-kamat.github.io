<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>PyTorch — An attempt to explain about Tensors</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">PyTorch — An attempt to explain about Tensors</h1>
</header>
<section data-field="subtitle" class="p-summary">
I have been learning and implementing Deep Learning models using Keras Framework. Recently, a friend of mine shared his work on GitHub…
</section>
<section data-field="body" class="e-content">
<section name="57f7" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2b8e" id="2b8e" class="graf graf--h3 graf--leading graf--title">PyTorch — An attempt to explain about Tensors</h3><p name="2dba" id="2dba" class="graf graf--p graf-after--h3">I have been learning and implementing Deep Learning models using Keras Framework. Recently, a friend of mine shared his work on GitHub from the course that he is been attending and mentioned they use PyTorch to build and train neural networks. That is when I started exploring PyTorch and I found that PyTorch makes it easier and cleaner in building Deep Learning models -much easier than TensorFlow!</p><h4 name="3de9" id="3de9" class="graf graf--h4 graf-after--p">A short Intro</h4><p name="c853" id="c853" class="graf graf--p graf-after--h4"><a href="https://pytorch.org/" data-href="https://pytorch.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch</a> is a python based deep learning framework and a scientific computing package. PyTorch was initially released in October 2016, primarily developed by Facebook’s AI Research lab and was based on <a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29" data-href="https://en.wikipedia.org/wiki/Torch_(machine_learning)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Torch</a> library (open source machine learning library, a scientific computing framework and a scripting language based on Lua programming language). As of 2018, Torch is no longer in active development and PyTorch is been actively developed and maintained.</p><h4 name="7b20" id="7b20" class="graf graf--h4 graf-after--p"><em class="markup--em markup--h4-em">PyTorch Tensors</em> — Super Important!</h4><p name="f552" id="f552" class="graf graf--p graf-after--h4">In math, there is a special name for the generalization of vectors and matrices to a higher dimensional space — Tensor. Tensors are the core data structures for PyTorch, used for building and training our neural networks. Tensors and their associated operations are similar to NumPy ndarrays (another python based scientific computing package), however unlike NumPy , Tensors use the power of GPUs (Graphical Processing Units) to accelerate computing, this helps PyTorch provide maximum flexibility and speed.</p><p name="a5c0" id="a5c0" class="graf graf--p graf-after--p">Tensor is an entity with a defined number of dimensions called an order (rank). <br><strong class="markup--strong markup--p-strong">Scalar</strong> can be considered as a rank-0-tensor.<br><strong class="markup--strong markup--p-strong">Vector</strong> can be introduced as a rank-1-tensor.<br><strong class="markup--strong markup--p-strong">Matrices</strong> can be considered as a rank-2-tensor.</p><figure name="1170" id="1170" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Dz4RQ6xt6iyMNVx_g7og_w.png" data-width="1564" data-height="481" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*Dz4RQ6xt6iyMNVx_g7og_w.png"><figcaption class="imageCaption">Tensors — Multidimensional Arrays.</figcaption></figure><p name="db20" id="db20" class="graf graf--p graf-after--figure">Let’s dive into few tensor operations and we can see how similar they are to NumPy</p><p name="4799" id="4799" class="graf graf--p graf-after--p">Before that, firstly, lets see the primary packages required to build and train neural networks using PyTorch</p><ul class="postList"><li name="1289" id="1289" class="graf graf--li graf-after--p">torch : The top-level PyTorch package and tensor library.</li><li name="7584" id="7584" class="graf graf--li graf-after--li">torch.nn : A subpackage that contains modules and extensible classes for building neural networks.</li><li name="ebf7" id="ebf7" class="graf graf--li graf-after--li">torch.autograd : A sub package that supports all the differentiable Tensor operations in PyTorch.</li><li name="fa0f" id="fa0f" class="graf graf--li graf-after--li">torch.nn.functional : A functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and convolution operations.</li><li name="029c" id="029c" class="graf graf--li graf-after--li">torch.optim : A sub package that contains standard optimization operations like SGD and Adam.</li><li name="5237" id="5237" class="graf graf--li graf-after--li">torch.utils : A sub package that contains utility classes like data sets and data loaders that make data preprocessing easier.</li><li name="5c7f" id="5c7f" class="graf graf--li graf-after--li">torchvision : This provides access to popular datasets, model architectures, and image transformations for computer vision.</li></ul><p name="7692" id="7692" class="graf graf--p graf-after--li"><em class="markup--em markup--p-em">Note — At present, torchvision package is separate from top-level torch package.</em></p><p name="ab96" id="ab96" class="graf graf--p graf-after--p">Tensors can be created in 4 ways:<br>- torch.Tensor(data)<br>- torch.tensor(data)<br>- torch.from_numpy(data)<br>- torch.as_tensor(data)</p><p name="f0bf" id="f0bf" class="graf graf--p graf-after--p">Let’s first import the torch library and create a tensor using above functions , like so</p><pre name="22ad" id="22ad" class="graf graf--pre graf-after--p">import torch</pre><pre name="8008" id="8008" class="graf graf--pre graf-after--pre">t = torch.tensor([<br>[1,1,1,1],<br>[2,2,2,2],<br>[3,3,3,3]<br>])</pre><pre name="2305" id="2305" class="graf graf--pre graf-after--pre">print(t)<br>print(&quot;\n Type of tensor&quot;, t.dtype)</pre><figure name="ac7f" id="ac7f" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Gs5PFiZIEYrgQe_1b83jNA.png" data-width="222" data-height="95" src="https://cdn-images-1.medium.com/max/800/1*Gs5PFiZIEYrgQe_1b83jNA.png"></figure><pre name="439f" id="439f" class="graf graf--pre graf-after--figure">t = torch.Tensor([<br>[1,1,1,1],<br>[2,2,2,2],<br>[3,3,3,3]<br>])</pre><pre name="e907" id="e907" class="graf graf--pre graf-after--pre">print(t)<br>print(&quot;\n Type of tensor&quot;, t.dtype)</pre><figure name="11a9" id="11a9" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*5sQ1RdFoV5P3tn3vfqZ-PQ.png" data-width="242" data-height="102" src="https://cdn-images-1.medium.com/max/800/1*5sQ1RdFoV5P3tn3vfqZ-PQ.png"></figure><p name="5d48" id="5d48" class="graf graf--p graf-after--figure">The first option is what we call a <em class="markup--em markup--p-em">factory function</em> that constructs <code class="markup--code markup--p-code">torch.tensor</code> objects and returns them to the caller and the second option with the uppercase <code class="markup--code markup--p-code">T</code> is the constructor of the <code class="markup--code markup--p-code">torch.Tensor</code> class.<br><code class="markup--code markup--p-code">torch.Tensor()</code> constructor uses the default <code class="markup--code markup--p-code">dtype</code> when building the tensor. The variants of tensor creation commands choose <code class="markup--code markup--p-code">dtype</code>based on the incoming data.</p><pre name="47b1" id="47b1" class="graf graf--pre graf-after--p"># Create a numpy array<br>data = np.array([<br>[1,1,1,1],<br>[2,2,2,2],<br>[3,3,3,3]<br>])</pre><pre name="c949" id="c949" class="graf graf--pre graf-after--pre"># Create a tensor using as_tensor() function<br>t1 = torch.as_tensor(data)</pre><pre name="3849" id="3849" class="graf graf--pre graf-after--pre"># Create a tensor using as_tensor() function<br>t2 = torch.from_numpy(data)</pre><p name="1c80" id="1c80" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">torch.Tensor()</code> and <code class="markup--code markup--p-code">torch.tensor()</code> <em class="markup--em markup--p-em">copy</em> their input data while <code class="markup--code markup--p-code">torch.as_tensor()</code> and <code class="markup--code markup--p-code">torch.from_numpy()</code> <em class="markup--em markup--p-em">share</em> their input data in memory with the original input object. Any changes done to the elements within original input or the new tensor — t1 or t2, is reflected within all the three tensors — data, t1 and t2.</p><blockquote name="9170" id="9170" class="graf graf--blockquote graf-after--p">Given all the above details the best option to create tensors is using<br><code class="markup--code markup--blockquote-code">torch.tensor()<br>torch.as_tensor()</code></blockquote><blockquote name="367e" id="367e" class="graf graf--blockquote graf-after--blockquote">The <code class="markup--code markup--blockquote-code">torch.tensor()</code> call is the sort of go-to call, while <code class="markup--code markup--blockquote-code">torch.as_tensor()</code> should be employed when tuning our code for performance.</blockquote><h4 name="e7b2" id="e7b2" class="graf graf--h4 graf-after--blockquote">Shape of Tensor</h4><p name="812f" id="812f" class="graf graf--p graf-after--h4">We can view the shape of the tensor in two ways:<br>- t.shape<br>- t.size()</p><figure name="5544" id="5544" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=18" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><figure name="dca7" id="dca7" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=19" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><p name="da39" id="da39" class="graf graf--p graf-after--figure">To determine the shape of the tensor, we first look at rows (i.e 3 in this case) and then column (i.e 4 in this case) , so the shape would be 3 X 4. The rank of the tensor would be 2 (because of two dimensions and also we can derive the rank using the number of ‘[[‘ (square brackets). We can also, get the rank of the tensor using len of shape</p><h4 name="8c5a" id="8c5a" class="graf graf--h4 graf-after--p">Number of Tensor Elements</h4><p name="9082" id="9082" class="graf graf--p graf-after--h4">We can get the number of elements within a tensor using two ways<br>* run a product operation on the shape of the tensor</p><figure name="0ed3" id="0ed3" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=34" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><p name="8946" id="8946" class="graf graf--p graf-after--figure">* there is also a direct function available with torch package — numel()</p><figure name="7a66" id="7a66" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=33" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><h4 name="6235" id="6235" class="graf graf--h4 graf-after--figure">Reshape</h4><p name="ad1f" id="ad1f" class="graf graf--p graf-after--h4"><em class="markup--em markup--p-em">Reshaping operations</em> are perhaps the most important type of tensor operations,we can change the shape of a tensor without the memory copying overhead. Tensor reshape is a key concept while building deep learning models, we can reshape the data using various method however we need to ensure that we maintain the same number of elements as in the original tensor. <br>i.e In the above scenario, we have 3 X 4 = 12 elements. We can reshape this to any dimension — (2,6) (6,2) (4,3) (12,1) (1,12) (2, 2, 3) etc, so that the total number of elements unchanged.</p><p name="6bba" id="6bba" class="graf graf--p graf-after--p">There are two methods for reshaping : view() and reshape() essentially perform same operation by returning a reshaped tensor without changing the original tensor in place.</p><ul class="postList"><li name="f06f" id="f06f" class="graf graf--li graf-after--p">view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to <a href="https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view" data-href="https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">some reason</a>, it just fails.</li><li name="f74d" id="f74d" class="graf graf--li graf-after--li">reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy</li></ul><figure name="4734" id="4734" class="graf graf--figure graf--iframe graf-after--li"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=24" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><figure name="7942" id="7942" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=23" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><p name="22b7" id="22b7" class="graf graf--p graf-after--figure">Views and reshape can infer the correct size, by passing in -1 PyTorch will infer the correct value</p><figure name="0b60" id="0b60" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/1&amp;cellId=27" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><h4 name="ce7f" id="ce7f" class="graf graf--h4 graf-after--figure">Changing Shape By Squeezing And Unsqueezing</h4><p name="5033" id="5033" class="graf graf--p graf-after--h4">Another we can change the shape of tensors is by <em class="markup--em markup--p-em">squeezing</em> and <em class="markup--em markup--p-em">unsqueezing</em> them.<br>- <em class="markup--em markup--p-em">Squeezing</em> a tensor removes the dimensions or axes that have a length of one.<br>- <em class="markup--em markup--p-em">Unsqueezing</em> a tensor adds a dimension with a length of one.<br>These functions allow us to expand or shrink the rank (number of dimensions) of our tensor.</p><figure name="8659" id="8659" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/2&amp;cellId=60" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><figure name="43cb" id="43cb" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/2&amp;cellId=62" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><figure name="396d" id="396d" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/2&amp;cellId=63" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><p name="3746" id="3746" class="graf graf--p graf-after--figure">Notice how the shape changes as we squeeze and unsqueeze the tensor.</p><h3 name="180a" id="180a" class="graf graf--h3 graf-after--p">Flatten a tensor</h3><p name="9646" id="9646" class="graf graf--p graf-after--h3">A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements, Flattening a tensor means to remove all of the dimensions except for one.</p><figure name="9758" id="9758" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/2&amp;cellId=65" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><figure name="eaab" id="eaab" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://jovian.ml/embed?url=https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment/v/2&amp;cellId=66" width="700" height="175" frameborder="0" scrolling="no"></iframe></figure><h4 name="842d" id="842d" class="graf graf--h4 graf-after--figure">Conclusion</h4><p name="189a" id="189a" class="graf graf--p graf-after--h4">We covered various tensor operations and their usage, however we have just scratched the surface there are many more functions and operations available in pytorch, which can be referenced from the official pytorch documentation — <a href="https://jovian.ml/outlink?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftensors.html" data-href="https://jovian.ml/outlink?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftensors.html" class="markup--anchor markup--p-anchor" rel="noopener noreferrer noopener" target="_blank">https://pytorch.org/docs/stable/tensors.html</a></p><h4 name="a762" id="a762" class="graf graf--h4 graf-after--p">References</h4><p name="df6a" id="df6a" class="graf graf--p graf-after--h4 graf--trailing"><a href="https://jovian.ml/outlink?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftensors.html" data-href="https://jovian.ml/outlink?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftensors.html" class="markup--anchor markup--p-anchor" rel="noopener noreferrer noopener noopener" target="_blank">https://pytorch.org/docs/stable/tensors.html</a><br><a href="https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG" data-href="https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG</a><br><a href="https://jovian.ml/forum/c/pytorch-zero-to-gans/18" data-href="https://jovian.ml/forum/c/pytorch-zero-to-gans/18" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://jovian.ml/forum/c/pytorch-zero-to-gans/18</a><br><a href="https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment" data-href="https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://jovian.ml/divya-r-kamat/01-tensor-operations-experiment</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@divya.r.kamat" class="p-author h-card">divya kamat</a> on <a href="https://medium.com/p/5831422ceddb"><time class="dt-published" datetime="2020-06-09T15:18:35.650Z">June 9, 2020</time></a>.</p><p><a href="https://medium.com/@divya.r.kamat/pytorch-an-attempt-to-explain-about-tensors-5831422ceddb" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 27, 2020.</p></footer></article></body></html>